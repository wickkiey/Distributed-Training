{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install contractions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import contractions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \nimport os \nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split\n\nimport string\nfrom tqdm import tqdm, tqdm_notebook\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nimport tensorflow_datasets as tfds\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import optimizers, losses, metrics, Model\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaConfig, TFRobertaModel,TFRobertaPreTrainedModel,TFRobertaForSequenceClassification, TFAutoModel, AutoTokenizer\nfrom transformers.modeling_tf_utils import get_initializer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.tpu.experimental.initialize_tpu_system(resolver)\ntpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Map(dict):\n   \n    def __init__(self, *args, **kwargs):\n        super(Map, self).__init__(*args, **kwargs)\n        for arg in args:\n            if isinstance(arg, dict):\n                for k, v in arg.iteritems():\n                    self[k] = v\n\n        if kwargs:\n            for k, v in kwargs.iteritems():\n                self[k] = v\n\n    def __getattr__(self, attr):\n        return self.get(attr)\n\n    def __setattr__(self, key, value):\n        self.__setitem__(key, value)\n\n    def __setitem__(self, key, value):\n        super(Map, self).__setitem__(key, value)\n        self.__dict__.update({key: value})\n\n    def __delattr__(self, item):\n        self.__delitem__(item)\n\n    def __delitem__(self, key):\n        super(Map, self).__delitem__(key)\n        del self.__dict__[key]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = Map()\nargs.model_name = 'roberta-base'\nargs.max_len=216\nargs.batch_size = 8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\nmain_df = df.drop(['id','url_legal','license','standard_error'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_decontraction(text):\n    expanded_words = []    \n    for word in text.split():      \n        expanded_words.append(contractions.fix(word))  \n    expanded_text = ' '.join(expanded_words)\n    return expanded_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translator = str.maketrans('', '', string.punctuation)\ndef remove_punct(text):\n    out = text.translate(translator)\n    return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_df['excerpt'] = main_df['excerpt'].apply(lambda x : apply_decontraction(x))\nmain_df['excerpt'] = main_df['excerpt'].apply(lambda x : remove_punct(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = train_test_split(main_df, test_size=0.15)\ntrain_df,valid_df = train_test_split(train_df, test_size=0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df_to_dataset(dataframe, shuffle=True):\n      dataframe = dataframe.copy()\n      labels = dataframe.pop('target')\n      ds = tf.data.Dataset.from_tensor_slices((dataframe['excerpt'], labels))\n      if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n      #ds = ds.batch(batch_size)\n      return ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = df_to_dataset(train_df, shuffle=True)\nvalid_ds = df_to_dataset(valid_df,shuffle=True)\ntest_ds = df_to_dataset(test_df,shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_ds),len(valid_ds),len(test_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_text_to_feature(text,max_len):\n    \n      return tokenizer.encode_plus(text, \n                add_special_tokens = False, # add [CLS], [SEP]\n                max_length = max_len, # max length of the text that can go to BERT\n                pad_to_max_length = True, # add [PAD] tokens\n                return_attention_mask = True, # add attention mask to not focus on pad tokens\n              )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_text_to_dict(input_ids, attention_masks, label):\n  return {\n      \"input_ids\": input_ids,      \n      \"attention_mask\": attention_masks,\n  }, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_text(ds, limit=-1):\n\n  # prepare list, so that we can build up final TensorFlow dataset from slices.\n      input_ids_list = []      \n      attention_mask_list = []\n      label_list = []\n\n      if (limit > 0):\n          ds = ds.take(limit)\n    \n      for text, label in tfds.as_numpy(ds):       \n\n        transformer_input = convert_text_to_feature(text.decode(),args.max_len)\n        \n        #print(transformer_input)\n        input_ids_list.append(transformer_input['input_ids'])        \n        attention_mask_list.append(transformer_input['attention_mask'])\n        label_list.append(label)\n\n      return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, label_list)).map(map_text_to_dict)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = args.batch_size * tpu_strategy.num_replicas_in_sync","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tpu_strategy.num_replicas_in_sync","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_train_encoded = encode_text(train_ds).shuffle(100).batch(BATCH_SIZE)\nds_val_encoded = encode_text(valid_ds).batch(BATCH_SIZE)\nds_test_encoded = encode_text(test_ds).batch(BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Model Initialization","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class CustomModel(TFRobertaPreTrainedModel):\n#     def __init__(self, config, *inputs, **kwargs):\n#         super(CustomModel, self).__init__(config, *inputs, **kwargs)\n#         self.num_labels = config.num_labels\n#         self.roberta = TFRobertaMainLayer(config, name=\"roberta\")\n#         self.dropout_1 = tf.keras.layers.Dropout(0.3)\n#         self.classifier = tf.keras.layers.Dense(units=config.num_labels,\n#                                                 name='regressor', \n#                                                 kernel_initializer=get_initializer(\n#                                                     config.initializer_range))\n\n#     def call(self, inputs, **kwargs):\n#         outputs = self.roberta(inputs, **kwargs)\n#         pooled_output = outputs[1]\n#         pooled_output = self.dropout_1(pooled_output, training=kwargs.get('training', False))\n#         logits = self.classifier(pooled_output)\n#         outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n#         return outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##predictions ##\ntest_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = RobertaConfig(dropout=0.2, attention_dropout=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getCustomModel():\n    roberta = 'roberta-base'\n    config = RobertaConfig(dropout=0.2, attention_dropout=0.2)\n    config.output_hidden_states = False\n    transformer_model = TFRobertaModel.from_pretrained(roberta, config = config)\n\n    input_ids_in = tf.keras.layers.Input(shape=(768,), name='input_token', dtype='int32')\n    input_masks_in = tf.keras.layers.Input(shape=(768,), name='masked_token', dtype='int32') \n\n    embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n    X = tf.keras.layers.GlobalMaxPool1D()(X)\n    X = tf.keras.layers.Dense(50, activation='relu')(X)\n    X = tf.keras.layers.Dropout(0.2)(X)\n    X = tf.keras.layers.Dense(1)(X)\n    model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n\n    for layer in model.layers[:3]:\n      layer.trainable = False\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = getCustomModel()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tpu_strategy.scope():\n    learning_rate = 0.0005\n    number_of_epochs=30\n    save_best_weights = 'best_weights.h5'\n    config = RobertaConfig.from_pretrained(args.model_name, num_labels=1)\n    #model = CustomModel.from_pretrained(args.model_name)\n    model = TFRobertaForSequenceClassification.from_pretrained(args.model_name, num_labels=1)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n    callbacks = [ModelCheckpoint(save_best_weights, monitor='val_loss', save_best_only=True),\n                EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto'),\n                ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=3)]\n    loss_fn = tf.keras.losses.MeanSquaredError(reduction='sum_over_batch_size')\n    model.compile(loss='mean_squared_error', optimizer=optimizer,metrics=[tf.keras.metrics.RootMeanSquaredError()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_history = model.fit(ds_train_encoded,\n                            epochs=number_of_epochs,\n                            validation_data=ds_val_encoded,\n                            callbacks = callbacks,\n                            verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for record,label in ds_test_encoded:\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(ds_test_encoded, verbose = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}